\section{Related Works}

\textbf{Dynamic Scene Reconstruction.} 
Dynamic scene reconstruction aims to recover the underlying representation of dynamic scenes from inputs like depth scans~\cite{curless1996volumetric, li2008global}, RGBD videos~\cite{newcombe2015dynamicfusion}, or monocular or multi-view videos~\cite{attal2023hyperreel, kratimenos2024dynmf, li2023dynibar, luiten2024dynamic, park2021nerfies, park2021hypernerf, pumarola2021d, wang2023flow, xian2021space, yu2023dylin, yu2024cogs, tretschk2021nonrigid, chu2022physics}. Recent advancements in dynamic scene modeling have involved the adaptation of novel scene representations, including Neural Radiance Fields (NeRF)~\cite{guan2022neurofluid, driess2023learning, li2023pac, li20223d, li2021neural, li2023dynibar, park2021nerfies, park2021hypernerf, pumarola2021d, tretschk2021non, wang2023flow, guo2023forward, cao2023hexplane, fridovich2023k, gao2022monocular, li2021neural, xian2021space, tretschk2021nonrigid, chu2022physics, peng2021CageNeRF} and 3D Gaussian splats~\cite{luiten2024dynamic, wu20244d, yang2024deformable, huang2024sc, kratimenos2024dynmf, lin2024gaussian, yu2024cogs, yang2023real, duan20244d}. D-NeRF~\cite{pumarola2021d} extends a canonical NeRF on dynamic scenes by optimizing a deformable field. Similarly, Deformable 3D-GS~\cite{yang2024deformable} optimizes a deformation field of each Gaussian kernel. Dynamic 3D-GS~\cite{luiten2024dynamic} optimizes the motion of Gaussian kernels for each frame to capture scene dynamics. 4D-GS~\cite{wu20244d} modulates 3D Gaussians with 4D neural voxels for dynamic multi-view synthesis. Although these methods achieve high-fidelity results in dynamic multi-view synthesis, they primarily focus on reconstructing scene appearance and geometry without capturing real-world dynamics, limiting their ability to support action-conditioned future predictions and interactive simulations.

\textbf{Physics-Based Simulation of Deformable Objects.} 
Another line of work incorporates physical simulators to perform system identification of physical parameters during reconstruction. Earlier methods relied on pre-scanned static objects and required clean point cloud observations~\cite{wang2015deformation, Qiao2021Differentiable, du2021diffpd, rojas2021differentiable, geilinger2020add, heiden2021disect, jatavallabhula2021gradsim, ma2022risp}. Most recent approaches build upon SDF~\cite{qiao2022neuphysics}, NeRF~\cite{feng2024pie, li2023pac, chen2022virtual} or Gaussian Splatting~\cite{zhang2024physdreamer, zhong2024reconstruction, xie2024physgaussian, jiang2024vr} to support more flexible physical digital twin reconstruction. 
Several works~\cite{feng2024pie, jiang2024vr, xie2024physgaussian} manually specify physics parameters, resulting in a mismatch between the simulation and real-world video observations. Other works~\cite{zhang2024physdreamer, li2023pac, chen2022virtual, zhong2024reconstruction, qiao2022neuphysics} attempt to estimate physical parameters from videos. However, they are often constrained to synthetic data, limited motion, or the need for dense viewpoints to accurately reconstruct static geometry, limiting their practical applicability.
The closest related work to ours is Spring-Gaus~\cite{zhong2024reconstruction}, which also utilizes a 3D Spring-Mass model for learning from videos. However, their physical model is overly regularized and violates real-world physics, lacking momentum conservation and realistic gravity. Moreover, Spring-Gaus requires dense viewpoint coverage to reconstruct the full geometry at the initial state, which is impractical in many real-world settings. The motions are also limited to tabletop collisions and lack action inputs, making Spring-Gaus unsuitable as a general dynamics model for downstream applications.


\textbf{Learning-Based Simulation of Deformable Objects.} 
Analytically modeling the dynamics of deformable objects is challenging due to the high complexity of the state space and the variability of physical properties. Recent works~\cite{wu2019learning, ma2023learning, xu2019densephysnet, evans2022context, chen2022comphy} have chosen to use neural network-based simulators to model object dynamics. Specifically, graph-based networks effectively learn the dynamics of various types of objects such as plasticine~\cite{shi2024robocraft, shi2023robocook}, cloth~\cite{pfaff2020learning, lin2022learning}, fluid~\cite{li2018learning, sanchez2020learning}, and stuffed animals~\cite{zhang2024dynamic}. GS-Dynamics~\cite{zhang2024dynamic} attempted to learn object dynamics directly from real-world videos using tracking and appearance priors from Dynamic Gaussians~\cite{luiten2024dynamic}, and generalized well to unseen actions. However, these learned models need extensive training samples and are often limited to specific environments with limited motion ranges. In contrast, our method requires only one interaction trial while achieving a broader range of motions.
