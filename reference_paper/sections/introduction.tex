\vspace{-20pt}
\section{Introduction}

The construction of interactive digital twins is essential for modeling the world and simulating future states, with applications in virtual reality, augmented reality, and robotic manipulation. A physically realistic digital twin (\ourabbr) should accurately capture the geometry, appearance, and physical properties of an object, allowing simulations that closely match observations in the real world. However, constructing such a representation from sparse observations remains a significant challenge.

The creation of digital twins for deformable objects has long been a challenging topic in the vision community. 
While dynamic 3D methods (e.g., dynamic NeRFs~\cite{driess2023learning, li2023pac, li20223d, li2021neural, li2023dynibar, park2021nerfies, park2021hypernerf, pumarola2021d, tretschk2021non, wang2023flow, guo2023forward, cao2023hexplane, fridovich2023k, gao2022monocular, xian2021space, tretschk2021nonrigid, chu2022physics, peng2021CageNeRF}, dynamic 3D Gaussians~\cite{luiten2024dynamic, wu20244d, yang2024deformable, huang2024sc, kratimenos2024dynmf, lin2024gaussian, yu2024cogs, yang2023real, duan20244d}) capture observed motion, appearance, and geometry from videos, they omit the underlying physics and are thus unsuitable for simulating outcomes in unseen interactions. 
While recent neural-based models~\cite{wu2019learning, ma2023learning, xu2019densephysnet, evans2022context, chen2022comphy, shi2024robocraft, shi2023robocook, pfaff2020learning, lin2022learning, li2018learning, sanchez2020learning, zhang2024dynamic} learn intuitive physics models from videos, they require large amounts of data and remain limited to specific objects or motions, whereas physics-driven approaches~\cite{zhang2024physdreamer, zhong2024reconstruction, xie2024physgaussian, feng2024pie, li2023pac, Qiao2021Differentiable, du2021diffpd} often rely on pre-scanned shapes or dense observations to mitigate ill-posedness.
Additionally, it requires dense viewpoint coverage and supports only limited motion types, making it unsuitable for general dynamics modeling.


In this work, we aim to build an interactive \ourabbr from sparse-viewpoint RGB-D video sequences, capturing object geometry, non-rigid dynamic physics, and appearance for realistic physical simulation and rendering. 
We model deformable object dynamics with a spring-mass-based representation, enabling efficient physical simulation and handling a wide range of common objects, such as ropes, stuffed animals, cloth, and delivery packages.
To address the challenges posed by sparse observations, we leverage shape priors and motion estimation from advanced 3D generative models~\cite{xiang2024structured} and vision foundation models~\cite{ren2024grounded, karaev2024cotracker3, rombach2022high} to estimate the topology, geometry, and physical parameters of our physical representation. 
Since some physical parameters (such as topology-related properties) are non-differentiable and optimizing them efficiently is non-trivial, we design a hierarchical sparse-to-dense optimization strategy. This strategy integrates zero-order optimization~\cite{hansen2006cma} for non-differentiable topology and sparse physical parameters (e.g., collision parameters and homogeneous spring stiffness), while employing first-order gradient-based optimization to refine dense spring stiffness and further optimize collision parameters.
For appearance modeling, we adopt a Gaussian blending strategy, initializing static Gaussians from sparse observations in the first frame using shape priors and deforming them with a linear blending algorithm to generate realistic dynamic appearances.

Our inverse modeling framework effectively constructs interactive \ourabbr from videos of objects under interaction. We create a real-world deformable object interaction dataset and evaluate our method on three key tasks: reconstruction and resimulation, future prediction, and generalization to unseen interactions. Both quantitative and qualitative results demonstrate that our reconstructed \ourabbr aligns accurately with real-world observations, achieves precise future predictions, and generates realistic simulations under diverse unseen interactions. Furthermore, the high computational efficiency of our physics simulator enables real-time dynamics and rendering of our constructed \ourabbr, facilitating multiple applications, including real-time interactive simulation and model-based robotic motion planning.
